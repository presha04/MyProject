# -*- coding: utf-8 -*-
"""Nltk Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pHAbbYa5lhjONc326BxqODm9Ao-sbuUi
"""

pip install nltk==3.5

pip install numpy matplotlib

from nltk.tokenize import sent_tokenize, word_tokenize

example_string = """
... Muad'Dib learned rapidly because his first training was in how to learn.
... And the first lesson of all was the basic trust that he could learn.
... It's shocking to find how many people do not believe they can learn,
... and how many more believe learning to be difficult."""

#import nlkt

import nltk
nltk.download('punkt')

#Tokenization example string

sent_tokenize(example_string)

#Tokenization by word

word_tokenize(example_string)

nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

worf_quote = "Sir, I protest. I am not a merry man!"

words_in_quote = word_tokenize(worf_quote)
>>> words_in_quote
['Sir', ',', 'protest', '.', 'merry', 'man', '!']

stop_words = set(stopwords.words("english"))

#Next, create an empty list to hold the words that make it past the filter:
filtered_list = []

for word in words_in_quote:
...    if word.casefold() not in stop_words:
...         filtered_list.append(word)

filtered_list = [
...     word for word in words_in_quote if word.casefold() not in stop_words
... ]

filtered_list
['Sir', ',', 'protest', '.', 'merry', 'man', '!']

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

string_for_stemming = """
... The crew of the USS Discovery discovered many discoveries.
... Discovering is what explorers do."""

words = word_tokenize(string_for_stemming)

words
['The',
 'crew',
 'of',
 'the',
 'USS',
 'Discovery',
 'discovered',
 'many',
 'discoveries',
 '.',
 'Discovering',
 'is',
 'what',
 'explorers',
 'do',
 '.']

stemmed_words = [stemmer.stem(word) for word in words]

#Take a look at what’s in stemmed_words:
stemmed_words
['the',
 'crew',
 'of',
 'the',
 'uss',
 'discoveri',
 'discov',
 'mani',
 'discoveri',
 '.',
 'discov',
 'is',
 'what',
 'explor',
 'do',
 '.']

from nltk.tokenize import word_tokenize

#Now create some text to tag. You can use this Carl Sagan quote:

sagan_quote = """
... If you wish to make an apple pie from scratch,
... you must first invent the universe."""

#Use word_tokenize to separate the words in that string and store them in a list:
words_in_sagan_quote = word_tokenize(sagan_quote)

import nltk
nltk.download('averaged_perceptron_tagger')

#Now call nltk.pos_tag() on your new list of words:
import nltk
nltk.pos_tag(words_in_sagan_quote)
[('If', 'IN'),
 ('you', 'PRP'),
 ('wish', 'VBP'),
 ('to', 'TO'),
 ('make', 'VB'),
 ('an', 'DT'),
 ('apple', 'NN'),
 ('pie', 'NN'),
 ('from', 'IN'),
 ('scratch', 'NN'),
 (',', ','),
 ('you', 'PRP'),
 ('must', 'MD'),
 ('first', 'VB'),
 ('invent', 'VB'),
 ('the', 'DT'),
 ('universe', 'NN'),
 ('.', '.')]

import nltk
nltk.download('tagsets')

nltk.help.upenn_tagset()

jabberwocky_excerpt = """
... 'Twas brillig, and the slithy toves did gyre and gimble in the wabe:
... all mimsy were the borogoves, and the mome raths outgrabe."""

#Use word_tokenize to separate the words in the excerpt and store them in a list:

words_in_excerpt = word_tokenize(jabberwocky_excerpt)

#Call nltk.pos_tag() on your new list of words:

nltk.pos_tag(words_in_excerpt)
[("'T", 'NN'),
 ('was', 'VBD'),
 ('brillig', 'VBN'),
 (',', ','),
 ('and', 'CC'),
 ('the', 'DT'),
 ('slithy', 'JJ'),
 ('toves', 'NNS'),
 ('did', 'VBD'),
 ('gyre', 'NN'),
 ('and', 'CC'),
 ('gimble', 'JJ'),
 ('in', 'IN'),
 ('the', 'DT'),
 ('wabe', 'NN'),
 (':', ':'),
 ('all', 'DT'),
 ('mimsy', 'NNS'),
 ('were', 'VBD'),
 ('the', 'DT'),
 ('borogoves', 'NNS'),
 (',', ','),
 ('and', 'CC'),
 ('the', 'DT'),
 ('mome', 'JJ'),
 ('raths', 'NNS'),
 ('outgrabe', 'RB'),
 ('.', '.')]

from nltk.stem import WordNetLemmatizer

#Create a lemmatizer to use:
lemmatizer = WordNetLemmatizer()

import nltk
nltk.download('wordnet')



#Let’s start with lemmatizing a plural noun:
lemmatizer.lemmatize("scarves")
'scarf'

string_for_lemmatizing = "The friends of DeSoto love scarves."

#Now tokenize that string by word:

words = word_tokenize(string_for_lemmatizing)

#Now tokenize that string by word:
words = word_tokenize(string_for_lemmatizing)

#Here’s your list of words:
words
['The',
 'friends',
 'of',
 'DeSoto',
 'love'
 'scarves',
 '.']

#Create a list containing all the words in words after they’ve been lemmatized:
lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

#Here’s the list you got:
lemmatized_words
['The',
 'friend',
 'of',
 'DeSoto',
 'love',
 'scarf',
 '.']

#That looks right. The plurals 'friends' and 'scarves' became the singulars 'friend' and 'scarf'.

#But what would happen if you lemmatized a word that looked very different from its lemma? Try lemmatizing "worst":

lemmatizer.lemmatize("worst")
'worst'

lemmatizer.lemmatize("worst", pos="a")
'bad'

from nltk.tokenize import word_tokenize

lotr_quote = "It's a dangerous business, Frodo, going out your door."

#Now tokenize that string by word:

words_in_lotr_quote = word_tokenize(lotr_quote)
words_in_lotr_quote
['It',
 "'s",
 'a',
 'dangerous',
 'business',
 ',',
 'Frodo',
 ',',
 'going',
 'out',
 'your',
 'door',
 '.']

#Now you’ve got a list of all of the words in lotr_quote.

#The next step is to tag those words by part of speech:
nltk.download("averaged_perceptron_tagger")
lotr_pos_tags = nltk.pos_tag(words_in_lotr_quote)
lotr_pos_tags
[('It', 'PRP'),
 ("'s", 'VBZ'),
 ('a', 'DT'),
 ('dangerous', 'JJ'),
 ('business', 'NN'),
 (',', ','),
 ('Frodo', 'NNP'),
 (',', ','),
 ('going', 'VBG'),
 ('out', 'RP'),
 ('your', 'PRP$'),
 ('door', 'NN'),
 ('.', '.')]

grammar = "Chunk: {<DT>?<JJ>*<NN>}"

chunk_parser = nltk.RegexpParser(grammar)

#Now try it out with your quote:
tree = chunk_parser.parse(lotr_pos_tags)

!pip install svgling
import svgling
svgling.draw_tree(tree)

import matplotlib
matplotlib.use('Agg')

lotr_pos_tags
[('It', 'PRP'),
 ("'s", 'VBZ'),
 ('a', 'DT'),
 ('dangerous', 'JJ'),
 ('business', 'NN'),
 (',', ','),
 ('Frodo', 'NNP'),
 (',', ','),
 ('going', 'VBG'),
 ('out', 'RP'),
 ('your', 'PRP$'),
 ('door', 'NN'),
 ('.', '.')]

#The next step is to create a grammar to determine what you want to include and exclude in your chunks. This time, you’re going to use more than one line because you’re going to have more than one rule. Because you’re using more than one line for the grammar, you’ll be using triple quotes ("""):

grammar = """
... Chunk: {<.*>+}
...        }<JJ>{"""

#Create a chunk parser with this grammar:
chunk_parser = nltk.RegexpParser(grammar)

#Now chunk your sentence with the chink you specified:
tree = chunk_parser.parse(lotr_pos_tags)

import svgling
svgling.draw_tree(tree)

nltk.download("maxent_ne_chunker")
nltk.download("words")
tree = nltk.ne_chunk(lotr_pos_tags)

nltk.download("maxent_ne_chunker")
nltk.download("words")
tree = nltk.ne_chunk(lotr_pos_tags)

#Getting Text to Analyze

#Tokenizing
#Tokenize and tag some text
import nltk
sentence = """At eight o'clock on Thursday morning
... Arthur didn't feel very good."""
tokens = nltk.word_tokenize(sentence)
tokens
['At', 'eight', "o'clock", 'on', 'Thursday', 'morning',
'Arthur', 'did', "n't", 'feel', 'very', 'good', '.']
tagged = nltk.pos_tag(tokens)
tagged[0:6]
[('At', 'IN'), ('eight', 'CD'), ("o'clock", 'JJ'), ('on', 'IN'),
('Thursday', 'NNP'), ('morning', 'NN')]

#Getting Text to Analyze
nltk.download("book")
from nltk.book import *

text1.concordance("man")

text1.concordance("woman")

#Here's an example of how to use concordance in NLTK (Natural Language Toolkit) in Python:
import nltk


# Load the text
text = text1

# Find concordance of a word
text.concordance("whale")

pip install matplotlib

# Commented out IPython magic to ensure Python compatibility.
#Here's an example of how to create a dispersion plot using NLTK and the same text from "Moby Dick" using python
import nltk
from nltk.book import *
# %matplotlib inline

# Load the text
text = text1

# Create a dispersion plot for the word "whale"
text.dispersion_plot(["whale"])

text1.dispersion_plot(
...     ["woman", "lady", "girl", "gal", "man", "gentleman", "boy", "guy"]
... )

#With a frequency distribution, you can check which words show up most frequently in your text. You’ll need to get started with an import:

import nltk
from nltk.book import *

# Load the text
text = text1

# Create a frequency distribution of the words in the text
fdist = FreqDist(text)

# Print the 50 most common words
print(fdist.most_common(50))

frequency_distribution = FreqDist(text)
print(frequency_distribution)

frequency_distribution.most_common(20)

meaningful_words = [
...     word for word in text if word.casefold() not in stop_words
... ]

frequency_distribution = FreqDist(meaningful_words)

#Take a look at the 20 most common words:
frequency_distribution.most_common(20)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

frequency_distribution.plot(20, cumulative=True)

